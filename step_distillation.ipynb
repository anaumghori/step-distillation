{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtwoP25tPp0aNw+3ERbVr9"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Install and Imports**"
      ],
      "metadata": {
        "id": "qcswBPPKYLbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets scikit-learn"
      ],
      "metadata": {
        "id": "Lk4YdmJ5YNBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW, get_scheduler\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "9FAYqEV1YSy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load dataset and Create splits**\n",
        "## Sample 10% of training data for validation set as done in the original paper"
      ],
      "metadata": {
        "id": "SfQ56tCXYYEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"anaumghori/cos_e-rationale\", split=['train', 'test'])\n",
        "train_dataset = ds[0]\n",
        "test_set = ds[1]\n",
        "\n",
        "val_size = int(0.1 * len(train_dataset))  # 10% for validation\n",
        "train_size = len(train_dataset) - val_size\n",
        "\n",
        "# Create the split\n",
        "train_set, val_set = random_split(\n",
        "    train_dataset,\n",
        "    [train_size, val_size],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")"
      ],
      "metadata": {
        "id": "_fPHgMYbYbEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Input: {train_set[0]['input']} | \\nLabel: {train_set[0]['label']}\")\n",
        "print(\"Rationale:\", train_set[0]['rationale'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9Q-tVeP-B9r",
        "outputId": "78172809-d948-4221-bae3-304f81fd89e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: The people wanted to do a demonstration, where did they decide to do it?\n",
            "Answer Choices:\n",
            "(a) supermarket\n",
            "(b) public place\n",
            "(c) demolition\n",
            "(d) space shuttle\n",
            "(e) roadblock | \n",
            "Label: public place\n",
            "Rationale: To determine the most suitable location for a demonstration, I need to consider the characteristics of each option provided. A supermarket is a commercial establishment focused on selling groceries and consumer goods; demonstrations typically take place in public spaces accessible to the general public. A public place, such as a park, square, or plaza, is intentionally designed for gatherings and meetings, allowing for free expression and assembly. Demolition refers to the process of destroying buildings or structures, which is unrelated to hosting a demonstration. Space shuttles are spacecraft used for space travel and are not available for ground-based demonstrations. Roadblocks are temporary barriers set up at intersections to control traffic, not locations for public gatherings. Among these options, a public place stands out as the ideal setting for a demonstration because it offers unrestricted access, ample space, and permission for organized assemblies. Public places are specifically designated for community interaction and have regulations in place to ensure orderly conduct during events. Therefore, the label \"public place\" correctly identifies the most appropriate location for a demonstration.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Custom Dataset Class**"
      ],
      "metadata": {
        "id": "K481dgQ_YzMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class T5StepByStepDistillationDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer, max_length=526):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.dataset[idx]\n",
        "\n",
        "        # Create input with both label and rationale prefixes\n",
        "        input_text_label = f\"[label] {example['input']}\"\n",
        "        input_text_rationale = f\"[rationale] {example['input']}\"\n",
        "\n",
        "        # Tokenize inputs for both tasks\n",
        "        input_encoding_label = self.tokenizer(\n",
        "            input_text_label,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        input_encoding_rationale = self.tokenizer(\n",
        "            input_text_rationale,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Tokenize targets for both tasks\n",
        "        label_encoding = self.tokenizer(\n",
        "            example['label'],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        rationale_encoding = self.tokenizer(\n",
        "            example['rationale'],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Replace padding token ID with -100\n",
        "        label_targets = label_encoding.input_ids.clone()\n",
        "        label_targets[label_targets == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        rationale_targets = rationale_encoding.input_ids.clone()\n",
        "        rationale_targets[rationale_targets == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return {\n",
        "            'input_ids_label': input_encoding_label.input_ids.squeeze(),\n",
        "            'attention_mask_label': input_encoding_label.attention_mask.squeeze(),\n",
        "            'labels_label': label_targets.squeeze(),\n",
        "            'input_ids_rationale': input_encoding_rationale.input_ids.squeeze(),\n",
        "            'attention_mask_rationale': input_encoding_rationale.attention_mask.squeeze(),\n",
        "            'labels_rationale': rationale_targets.squeeze()\n",
        "        }"
      ],
      "metadata": {
        "id": "k6VMvCDNarNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class T5TestDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer, max_length=128):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.dataset[idx]\n",
        "\n",
        "        # Only predict labels for testing\n",
        "        input_text = f\"[label] {example['input']}\"\n",
        "\n",
        "        # Tokenize inputs\n",
        "        input_encoding = self.tokenizer(\n",
        "            input_text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Tokenize labels\n",
        "        label_encoding = self.tokenizer(\n",
        "            example['label'],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Replace padding token ID with -100\n",
        "        labels = label_encoding.input_ids.clone()\n",
        "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_encoding.input_ids.squeeze(),\n",
        "            'attention_mask': input_encoding.attention_mask.squeeze(),\n",
        "            'labels': labels.squeeze(),\n",
        "            'reference': example['label']  # Store original label for evaluation\n",
        "        }"
      ],
      "metadata": {
        "id": "braRYmmN3Sj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Validation Function**"
      ],
      "metadata": {
        "id": "I7EZEnA5Y61h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_model(model, val_loader, device, lambda_value=0.2):\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            # Process label task\n",
        "            input_ids_label = batch['input_ids_label'].to(device)\n",
        "            attention_mask_label = batch['attention_mask_label'].to(device)\n",
        "            labels_label = batch['labels_label'].to(device)\n",
        "\n",
        "            # Process rationale task\n",
        "            input_ids_rationale = batch['input_ids_rationale'].to(device)\n",
        "            attention_mask_rationale = batch['attention_mask_rationale'].to(device)\n",
        "            labels_rationale = batch['labels_rationale'].to(device)\n",
        "\n",
        "            # Get losses for both tasks\n",
        "            outputs_label = model(\n",
        "                input_ids=input_ids_label,\n",
        "                attention_mask=attention_mask_label,\n",
        "                labels=labels_label\n",
        "            )\n",
        "\n",
        "            outputs_rationale = model(\n",
        "                input_ids=input_ids_rationale,\n",
        "                attention_mask=attention_mask_rationale,\n",
        "                labels=labels_rationale\n",
        "            )\n",
        "\n",
        "            # Calculate combined loss: L = Llabel + λLrationale\n",
        "            loss_label = outputs_label.loss\n",
        "            loss_rationale = outputs_rationale.loss\n",
        "            combined_loss = loss_label + lambda_value * loss_rationale\n",
        "\n",
        "            total_val_loss += combined_loss.item()\n",
        "\n",
        "    return total_val_loss / len(val_loader)"
      ],
      "metadata": {
        "id": "U9pFSUTE3jRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train the model**"
      ],
      "metadata": {
        "id": "0MXc_itNZI_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step_by_step_distillation(train_set, val_set, model_name=\"google-t5/t5-small\", max_length=526,\n",
        "                                    batch_size=16, num_epochs=25, learning_rate=3e-5, weight_decay=0.01,\n",
        "                                    lambda_value=0.2):\n",
        "    # Load model and tokenizer\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Prepare datasets for combined training\n",
        "    train_dataset = T5StepByStepDistillationDataset(train_set, tokenizer, max_length)\n",
        "    val_dataset = T5StepByStepDistillationDataset(val_set, tokenizer, max_length)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Initialize optimizer and scheduler\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    total_steps = len(train_loader) * num_epochs\n",
        "\n",
        "    lr_scheduler = get_scheduler(\n",
        "        \"linear\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    # Store the best model\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Training - Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            # Process label task\n",
        "            input_ids_label = batch['input_ids_label'].to(device)\n",
        "            attention_mask_label = batch['attention_mask_label'].to(device)\n",
        "            labels_label = batch['labels_label'].to(device)\n",
        "\n",
        "            # Process rationale task\n",
        "            input_ids_rationale = batch['input_ids_rationale'].to(device)\n",
        "            attention_mask_rationale = batch['attention_mask_rationale'].to(device)\n",
        "            labels_rationale = batch['labels_rationale'].to(device)\n",
        "\n",
        "            # Get losses for both tasks\n",
        "            outputs_label = model(\n",
        "                input_ids=input_ids_label,\n",
        "                attention_mask=attention_mask_label,\n",
        "                labels=labels_label\n",
        "            )\n",
        "\n",
        "            outputs_rationale = model(\n",
        "                input_ids=input_ids_rationale,\n",
        "                attention_mask=attention_mask_rationale,\n",
        "                labels=labels_rationale\n",
        "            )\n",
        "\n",
        "            # Calculate combined loss: L = Llabel + λLrationale\n",
        "            loss_label = outputs_label.loss\n",
        "            loss_rationale = outputs_rationale.loss\n",
        "            combined_loss = loss_label + lambda_value * loss_rationale\n",
        "\n",
        "            total_train_loss += combined_loss.item()\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            combined_loss.backward()\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "\n",
        "            progress_bar.set_postfix({\n",
        "                \"loss\": combined_loss.item(),\n",
        "                \"label_loss\": loss_label.item(),\n",
        "                \"rationale_loss\": loss_rationale.item()\n",
        "            })\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1} - Average training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        print(\"Running validation...\")\n",
        "        val_loss = validate_model(model, val_loader, device, lambda_value)\n",
        "        print(f\"Epoch {epoch+1} - Validation loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_state = model.state_dict().copy()\n",
        "            print(f\"New best model saved with validation loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Load best model for final evaluation\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "        print(f\"Loaded best model with validation loss: {best_val_loss:.4f}\")\n",
        "\n",
        "    print(\"Training completed!\")\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "KusY6TXQ3qVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = train_step_by_step_distillation(train_set, val_set)"
      ],
      "metadata": {
        "id": "8ofD7IT1ZQyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate the model**"
      ],
      "metadata": {
        "id": "UwefoqihZXhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, tokenizer, test_dataset, max_length=128, batch_size=16):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Prepare test dataset\n",
        "    test_dataset_processed = T5TestDataset(test_dataset, tokenizer, max_length)\n",
        "    test_loader = DataLoader(test_dataset_processed, batch_size=batch_size)\n",
        "\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    print(\"Starting evaluation...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "            # Generate predictions\n",
        "            outputs = model.generate(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                max_length=max_length\n",
        "            )\n",
        "\n",
        "            # Decode predictions\n",
        "            preds = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "            refs = batch[\"reference\"]\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            references.extend(refs)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(references, predictions) * 100\n",
        "    return accuracy, predictions, references"
      ],
      "metadata": {
        "id": "dvItD8sD72ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy, predictions, references = evaluate_model(model, tokenizer, test_set)"
      ],
      "metadata": {
        "id": "Z5wOPGwBZdTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Results**"
      ],
      "metadata": {
        "id": "c0dEssLAZkBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_evaluation_results(accuracy, predictions, references, num_samples=3):\n",
        "    print(f\"Final Evaluation Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    # Display some examples\n",
        "    print(\"\\nSample predictions:\")\n",
        "    indices = list(range(len(predictions)))\n",
        "    sample_indices = indices[:num_samples]\n",
        "\n",
        "    for i in sample_indices:\n",
        "        print(f\"Reference: {references[i]}\")\n",
        "        print(f\"Prediction: {predictions[i]}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "# Display results\n",
        "display_evaluation_results(accuracy, predictions, references, num_samples=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmDaUcgTKGZQ",
        "outputId": "f2c27b75-4435-42bd-c779-227df3f8e902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Evaluation Accuracy: 52.60%\n",
            "\n",
            "Reference: wooded area\n",
            "Prediction: wooded area\n",
            "--------------------------------------------------\n",
            "Reference: go downtown\n",
            "Prediction: go downtown\n",
            "--------------------------------------------------\n",
            "Reference: play tag\n",
            "Prediction: play tag\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}